{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57cUBU_kW8YI"
      },
      "source": [
        "# Quick Tour\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/lavague-ai/LaVague/blob/main/docs/docs/get-started/quick-tour-notebook/quick-tour.ipynb\">\n",
        "<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBcpVSR6fwT-"
      },
      "source": [
        "## Pre-requisites\n",
        "\n",
        "**Note**: We use OpenAI's models, for the embedding, LLM and Vision model. You will need to set the OPENAI_API_KEY variable in your local environment with a valid API key for this example to work.\n",
        "\n",
        "If you don't have an OpenAI API key, please get one here: https://platform.openai.com/docs/quickstart/developer-quickstart"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAFBONJcd8mI"
      },
      "source": [
        "# Installation\n",
        "\n",
        "We start by download LaVague."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "E6TVyo2uGrJq",
        "outputId": "1bb33050-6316-416c-b68c-83da45019a8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lavague\n",
            "  Downloading lavague-1.1.19-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting lavague-contexts-openai<0.3.0,>=0.2.0 (from lavague)\n",
            "  Downloading lavague_contexts_openai-0.2.4-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting lavague-core<0.3.0,>=0.2.31 (from lavague)\n",
            "  Downloading lavague_core-0.2.35-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting lavague-drivers-selenium<0.3.0,>=0.2.12 (from lavague)\n",
            "  Downloading lavague_drivers_selenium-0.2.15-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting lavague-gradio<0.3.0,>=0.2.8 (from lavague)\n",
            "  Downloading lavague_gradio-0.2.8-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting llama-index-embeddings-azure-openai==0.1.11 (from lavague-contexts-openai<0.3.0,>=0.2.0->lavague)\n",
            "  Downloading llama_index_embeddings_azure_openai-0.1.11-py3-none-any.whl.metadata (804 bytes)\n",
            "Collecting llama-index-embeddings-openai<0.2.0,>=0.1.9 (from lavague-contexts-openai<0.3.0,>=0.2.0->lavague)\n",
            "  Downloading llama_index_embeddings_openai-0.1.11-py3-none-any.whl.metadata (655 bytes)\n",
            "Collecting llama-index-llms-azure-openai==0.1.10 (from lavague-contexts-openai<0.3.0,>=0.2.0->lavague)\n",
            "  Downloading llama_index_llms_azure_openai-0.1.10-py3-none-any.whl.metadata (787 bytes)\n",
            "Collecting llama-index-llms-openai<0.2.0,>=0.1.9 (from lavague-contexts-openai<0.3.0,>=0.2.0->lavague)\n",
            "  Downloading llama_index_llms_openai-0.1.31-py3-none-any.whl.metadata (650 bytes)\n",
            "Collecting llama-index-multi-modal-llms-azure-openai==0.1.4 (from lavague-contexts-openai<0.3.0,>=0.2.0->lavague)\n",
            "  Downloading llama_index_multi_modal_llms_azure_openai-0.1.4-py3-none-any.whl.metadata (820 bytes)\n",
            "Collecting llama-index-multi-modal-llms-openai<0.2.0,>=0.1.6 (from lavague-contexts-openai<0.3.0,>=0.2.0->lavague)\n",
            "  Downloading llama_index_multi_modal_llms_openai-0.1.9-py3-none-any.whl.metadata (728 bytes)\n",
            "Collecting llama-index-core<0.11.0,>=0.10.11.post1 (from llama-index-embeddings-azure-openai==0.1.11->lavague-contexts-openai<0.3.0,>=0.2.0->lavague)\n",
            "  Downloading llama_index_core-0.10.68.post1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting azure-identity<2.0.0,>=1.15.0 (from llama-index-llms-azure-openai==0.1.10->lavague-contexts-openai<0.3.0,>=0.2.0->lavague)\n",
            "  Downloading azure_identity-1.21.0-py3-none-any.whl.metadata (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index-llms-azure-openai==0.1.10->lavague-contexts-openai<0.3.0,>=0.2.0->lavague) (0.28.1)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from lavague-core<0.3.0,>=0.2.31->lavague) (6.0.2)\n",
            "Requirement already satisfied: ipython<8.0.0,>=7.34.0 in /usr/local/lib/python3.11/dist-packages (from lavague-core<0.3.0,>=0.2.31->lavague) (7.34.0)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.23.0 in /usr/local/lib/python3.11/dist-packages (from lavague-core<0.3.0,>=0.2.31->lavague) (4.23.0)\n",
            "Collecting langchain<0.2.0,>=0.1.20 (from lavague-core<0.3.0,>=0.2.31->lavague)\n",
            "  Downloading langchain-0.1.20-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting llama-index==0.10.56 (from lavague-core<0.3.0,>=0.2.31->lavague)\n",
            "  Downloading llama_index-0.10.56-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting llama-index-retrievers-bm25<0.2.0,>=0.1.3 (from lavague-core<0.3.0,>=0.2.31->lavague)\n",
            "  Downloading llama_index_retrievers_bm25-0.1.5-py3-none-any.whl.metadata (700 bytes)\n",
            "Requirement already satisfied: lxml<6.0.0,>=5.1.1 in /usr/local/lib/python3.11/dist-packages (from lavague-core<0.3.0,>=0.2.31->lavague) (5.3.1)\n",
            "Collecting lxml-html-clean<0.2.0,>=0.1.1 (from lavague-core<0.3.0,>=0.2.31->lavague)\n",
            "  Downloading lxml_html_clean-0.1.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from lavague-core<0.3.0,>=0.2.31->lavague) (1.1.0)\n",
            "Requirement already satisfied: seaborn<0.14.0,>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from lavague-core<0.3.0,>=0.2.31->lavague) (0.13.2)\n",
            "Collecting tenacity<8.4.0,>=8.2.0 (from lavague-core<0.3.0,>=0.2.31->lavague)\n",
            "  Downloading tenacity-8.3.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting trafilatura<2.0.0,>=1.9.0 (from lavague-core<0.3.0,>=0.2.31->lavague)\n",
            "  Downloading trafilatura-1.12.2-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting llama-index-agent-openai<0.3.0,>=0.1.4 (from llama-index==0.10.56->lavague-core<0.3.0,>=0.2.31->lavague)\n",
            "  Downloading llama_index_agent_openai-0.2.9-py3-none-any.whl.metadata (729 bytes)\n",
            "Collecting llama-index-cli<0.2.0,>=0.1.2 (from llama-index==0.10.56->lavague-core<0.3.0,>=0.2.31->lavague)\n",
            "  Downloading llama_index_cli-0.1.13-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting llama-index-core<0.11.0,>=0.10.11.post1 (from llama-index-embeddings-azure-openai==0.1.11->lavague-contexts-openai<0.3.0,>=0.2.0->lavague)\n",
            "  Downloading llama_index_core-0.10.56-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting llama-index-indices-managed-llama-cloud>=0.2.0 (from llama-index==0.10.56->lavague-core<0.3.0,>=0.2.31->lavague)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.6.8-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama-index==0.10.56->lavague-core<0.3.0,>=0.2.31->lavague)\n",
            "  Downloading llama_index_legacy-0.9.48.post4-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting llama-index-program-openai<0.2.0,>=0.1.3 (from llama-index==0.10.56->lavague-core<0.3.0,>=0.2.31->lavague)\n",
            "  Downloading llama_index_program_openai-0.1.7-py3-none-any.whl.metadata (760 bytes)\n",
            "Collecting llama-index-question-gen-openai<0.2.0,>=0.1.2 (from llama-index==0.10.56->lavague-core<0.3.0,>=0.2.31->lavague)\n",
            "  Downloading llama_index_question_gen_openai-0.1.3-py3-none-any.whl.metadata (785 bytes)\n",
            "Collecting llama-index-readers-file<0.2.0,>=0.1.4 (from llama-index==0.10.56->lavague-core<0.3.0,>=0.2.31->lavague)\n",
            "  Downloading llama_index_readers_file-0.1.33-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting llama-index-readers-llama-parse>=0.1.2 (from llama-index==0.10.56->lavague-core<0.3.0,>=0.2.31->lavague)\n",
            "  Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai==0.1.11->lavague-contexts-openai<0.3.0,>=0.2.0->lavague) (2.0.38)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai==0.1.11->lavague-contexts-openai<0.3.0,>=0.2.0->lavague) (3.11.13)\n",
            "Collecting dataclasses-json (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai==0.1.11->lavague-contexts-openai<0.3.0,>=0.2.0->lavague)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai==0.1.11->lavague-contexts-openai<0.3.0,>=0.2.0->lavague) (1.2.18)\n",
            "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai==0.1.11->lavague-contexts-openai<0.3.0,>=0.2.0->lavague)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai==0.1.11->lavague-contexts-openai<0.3.0,>=0.2.0->lavague) (2024.10.0)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai==0.1.11->lavague-contexts-openai<0.3.0,>=0.2.0->lavague) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai==0.1.11->lavague-contexts-openai<0.3.0,>=0.2.0->lavague) (3.4.2)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai==0.1.11->lavague-contexts-openai<0.3.0,>=0.2.0->lavague) (3.9.1)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai==0.1.11->lavague-contexts-openai<0.3.0,>=0.2.0->lavague) (1.26.4)\n",
            "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai==0.1.11->lavague-contexts-openai<0.3.0,>=0.2.0->lavague) (1.61.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai==0.1.11->lavague-contexts-openai<0.3.0,>=0.2.0->lavague) (2.2.2)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai==0.1.11->lavague-contexts-openai<0.3.0,>=0.2.0->lavague) (11.1.0)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai==0.1.11->lavague-contexts-openai<0.3.0,>=0.2.0->lavague) (2.32.3)\n",
            "Collecting tiktoken>=0.3.3 (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai==0.1.11->lavague-contexts-openai<0.3.0,>=0.2.0->lavague)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai==0.1.11->lavague-contexts-openai<0.3.0,>=0.2.0->lavague) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai==0.1.11->lavague-contexts-openai<0.3.0,>=0.2.0->lavague) (4.12.2)\n",
            "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai==0.1.11->lavague-contexts-openai<0.3.0,>=0.2.0->lavague)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai==0.1.11->lavague-contexts-openai<0.3.0,>=0.2.0->lavague) (1.17.2)\n",
            "Collecting selenium<5.0.0,>=4.18.1 (from lavague-drivers-selenium<0.3.0,>=0.2.12->lavague)\n",
            "  Downloading selenium-4.29.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting gradio==4.39.0 (from lavague-gradio<0.3.0,>=0.2.8->lavague)\n",
            "  Downloading gradio-4.39.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague) (3.7.1)\n",
            "Collecting fastapi (from gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague)\n",
            "  Downloading fastapi-0.115.11-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.1.1 (from gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague)\n",
            "  Downloading gradio_client-1.1.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.11/dist-packages (from gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague) (0.28.1)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.11/dist-packages (from gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague) (6.5.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague) (3.1.6)\n",
            "Collecting markupsafe~=2.0 (from gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague)\n",
            "  Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague) (3.10.0)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague) (3.10.15)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague) (24.2)\n",
            "Collecting pillow>=9.0.0 (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai==0.1.11->lavague-contexts-openai<0.3.0,>=0.2.0->lavague)\n",
            "  Downloading pillow-10.4.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague) (2.10.6)\n",
            "Collecting pydub (from gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.9 (from gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting ruff>=0.2.2 (from gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague)\n",
            "  Downloading ruff-0.9.10-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague) (0.15.2)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague) (2.3.0)\n",
            "Collecting uvicorn>=0.14.0 (from gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting websockets<12.0,>=10.0 (from gradio-client==1.1.1->gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague)\n",
            "  Downloading websockets-11.0.3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython<8.0.0,>=7.34.0->lavague-core<0.3.0,>=0.2.31->lavague) (75.1.0)\n",
            "Collecting jedi>=0.16 (from ipython<8.0.0,>=7.34.0->lavague-core<0.3.0,>=0.2.31->lavague)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython<8.0.0,>=7.34.0->lavague-core<0.3.0,>=0.2.31->lavague) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython<8.0.0,>=7.34.0->lavague-core<0.3.0,>=0.2.31->lavague) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from ipython<8.0.0,>=7.34.0->lavague-core<0.3.0,>=0.2.31->lavague) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython<8.0.0,>=7.34.0->lavague-core<0.3.0,>=0.2.31->lavague) (3.0.50)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython<8.0.0,>=7.34.0->lavague-core<0.3.0,>=0.2.31->lavague) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython<8.0.0,>=7.34.0->lavague-core<0.3.0,>=0.2.31->lavague) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython<8.0.0,>=7.34.0->lavague-core<0.3.0,>=0.2.31->lavague) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython<8.0.0,>=7.34.0->lavague-core<0.3.0,>=0.2.31->lavague) (4.9.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.23.0->lavague-core<0.3.0,>=0.2.31->lavague) (25.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.23.0->lavague-core<0.3.0,>=0.2.31->lavague) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.23.0->lavague-core<0.3.0,>=0.2.31->lavague) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.23.0->lavague-core<0.3.0,>=0.2.31->lavague) (0.23.1)\n",
            "Collecting langchain-community<0.1,>=0.0.38 (from langchain<0.2.0,>=0.1.20->lavague-core<0.3.0,>=0.2.31->lavague)\n",
            "  Downloading langchain_community-0.0.38-py3-none-any.whl.metadata (8.7 kB)\n",
            "Collecting langchain-core<0.2.0,>=0.1.52 (from langchain<0.2.0,>=0.1.20->lavague-core<0.3.0,>=0.2.31->lavague)\n",
            "  Downloading langchain_core-0.1.53-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting langchain-text-splitters<0.1,>=0.0.1 (from langchain<0.2.0,>=0.1.20->lavague-core<0.3.0,>=0.2.31->lavague)\n",
            "  Downloading langchain_text_splitters-0.0.2-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain<0.2.0,>=0.1.20->lavague-core<0.3.0,>=0.2.31->lavague)\n",
            "  Downloading langsmith-0.1.147-py3-none-any.whl.metadata (14 kB)\n",
            "INFO: pip is looking at multiple versions of llama-index-llms-openai to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting llama-index-llms-openai<0.2.0,>=0.1.9 (from lavague-contexts-openai<0.3.0,>=0.2.0->lavague)\n",
            "  Downloading llama_index_llms_openai-0.1.30-py3-none-any.whl.metadata (650 bytes)\n",
            "  Downloading llama_index_llms_openai-0.1.29-py3-none-any.whl.metadata (650 bytes)\n",
            "  Downloading llama_index_llms_openai-0.1.28-py3-none-any.whl.metadata (650 bytes)\n",
            "  Downloading llama_index_llms_openai-0.1.27-py3-none-any.whl.metadata (610 bytes)\n",
            "  Downloading llama_index_llms_openai-0.1.26-py3-none-any.whl.metadata (610 bytes)\n",
            "Collecting rank-bm25<0.3.0,>=0.2.2 (from llama-index-retrievers-bm25<0.2.0,>=0.1.3->lavague-core<0.3.0,>=0.2.31->lavague)\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting trio~=0.17 (from selenium<5.0.0,>=4.18.1->lavague-drivers-selenium<0.3.0,>=0.2.12->lavague)\n",
            "  Downloading trio-0.29.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting trio-websocket~=0.9 (from selenium<5.0.0,>=4.18.1->lavague-drivers-selenium<0.3.0,>=0.2.12->lavague)\n",
            "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.11/dist-packages (from selenium<5.0.0,>=4.18.1->lavague-drivers-selenium<0.3.0,>=0.2.12->lavague) (2025.1.31)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.11/dist-packages (from selenium<5.0.0,>=4.18.1->lavague-drivers-selenium<0.3.0,>=0.2.12->lavague) (1.8.0)\n",
            "Collecting courlan>=1.2.0 (from trafilatura<2.0.0,>=1.9.0->lavague-core<0.3.0,>=0.2.31->lavague)\n",
            "  Downloading courlan-1.3.2-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting htmldate>=1.8.1 (from trafilatura<2.0.0,>=1.9.0->lavague-core<0.3.0,>=0.2.31->lavague)\n",
            "  Downloading htmldate-1.9.3-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting justext>=3.0.1 (from trafilatura<2.0.0,>=1.9.0->lavague-core<0.3.0,>=0.2.31->lavague)\n",
            "  Downloading justext-3.0.2-py2.py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: charset-normalizer>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from trafilatura<2.0.0,>=1.9.0->lavague-core<0.3.0,>=0.2.31->lavague) (3.4.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai==0.1.11->lavague-contexts-openai<0.3.0,>=0.2.0->lavague) (2.5.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai==0.1.11->lavague-contexts-openai<0.3.0,>=0.2.0->lavague) (1.3.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai==0.1.11->lavague-contexts-openai<0.3.0,>=0.2.0->lavague) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai==0.1.11->lavague-contexts-openai<0.3.0,>=0.2.0->lavague) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai==0.1.11->lavague-contexts-openai<0.3.0,>=0.2.0->lavague) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai==0.1.11->lavague-contexts-openai<0.3.0,>=0.2.0->lavague) (1.18.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague) (1.3.1)\n",
            "Collecting azure-core>=1.31.0 (from azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai==0.1.10->lavague-contexts-openai<0.3.0,>=0.2.0->lavague)\n",
            "  Downloading azure_core-1.32.0-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: cryptography>=2.5 in /usr/local/lib/python3.11/dist-packages (from azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai==0.1.10->lavague-contexts-openai<0.3.0,>=0.2.0->lavague) (43.0.3)\n",
            "Collecting msal>=1.30.0 (from azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai==0.1.10->lavague-contexts-openai<0.3.0,>=0.2.0->lavague)\n",
            "  Downloading msal-1.31.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting msal-extensions>=1.2.0 (from azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai==0.1.10->lavague-contexts-openai<0.3.0,>=0.2.0->lavague)\n",
            "  Downloading msal_extensions-1.2.0-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: babel>=2.16.0 in /usr/local/lib/python3.11/dist-packages (from courlan>=1.2.0->trafilatura<2.0.0,>=1.9.0->lavague-core<0.3.0,>=0.2.31->lavague) (2.17.0)\n",
            "Collecting tld>=0.13 (from courlan>=1.2.0->trafilatura<2.0.0,>=1.9.0->lavague-core<0.3.0,>=0.2.31->lavague)\n",
            "  Downloading tld-0.13-py2.py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai==0.1.11->lavague-contexts-openai<0.3.0,>=0.2.0->lavague)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting dateparser>=1.1.2 (from htmldate>=1.8.1->trafilatura<2.0.0,>=1.9.0->lavague-core<0.3.0,>=0.2.31->lavague)\n",
            "  Downloading dateparser-1.2.1-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting python-dateutil>=2.9.0.post0 (from htmldate>=1.8.1->trafilatura<2.0.0,>=1.9.0->lavague-core<0.3.0,>=0.2.31->lavague)\n",
            "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-llms-azure-openai==0.1.10->lavague-contexts-openai<0.3.0,>=0.2.0->lavague) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index-llms-azure-openai==0.1.10->lavague-contexts-openai<0.3.0,>=0.2.0->lavague) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.3->gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague) (3.17.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython<8.0.0,>=7.34.0->lavague-core<0.3.0,>=0.2.31->lavague) (0.8.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.2.0,>=0.1.52->langchain<0.2.0,>=0.1.20->lavague-core<0.3.0,>=0.2.31->lavague) (1.33)\n",
            "Collecting packaging (from gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague)\n",
            "  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain<0.2.0,>=0.1.20->lavague-core<0.3.0,>=0.2.31->lavague) (1.0.0)\n",
            "Collecting llama-cloud<0.2.0,>=0.1.13 (from llama-index-indices-managed-llama-cloud>=0.2.0->llama-index==0.10.56->lavague-core<0.3.0,>=0.2.31->lavague)\n",
            "  Downloading llama_cloud-0.1.14-py3-none-any.whl.metadata (902 bytes)\n",
            "INFO: pip is looking at multiple versions of llama-index-indices-managed-llama-cloud to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting llama-index-indices-managed-llama-cloud>=0.2.0 (from llama-index==0.10.56->lavague-core<0.3.0,>=0.2.31->lavague)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.6.7-py3-none-any.whl.metadata (3.6 kB)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.6.6-py3-none-any.whl.metadata (3.6 kB)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.6.5-py3-none-any.whl.metadata (3.6 kB)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.6.4-py3-none-any.whl.metadata (3.6 kB)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.6.3-py3-none-any.whl.metadata (3.8 kB)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.6.2-py3-none-any.whl.metadata (3.8 kB)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.6.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "INFO: pip is still looking at multiple versions of llama-index-indices-managed-llama-cloud to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.6.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.5.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.4.2-py3-none-any.whl.metadata (3.8 kB)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.4.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.4.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.3.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.3.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.2.7-py3-none-any.whl.metadata (3.8 kB)\n",
            "INFO: pip is looking at multiple versions of llama-index-program-openai to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting llama-index-program-openai<0.2.0,>=0.1.3 (from llama-index==0.10.56->lavague-core<0.3.0,>=0.2.31->lavague)\n",
            "  Downloading llama_index_program_openai-0.1.6-py3-none-any.whl.metadata (715 bytes)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index==0.10.56->lavague-core<0.3.0,>=0.2.31->lavague) (4.13.3)\n",
            "Collecting pypdf<5.0.0,>=4.0.1 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index==0.10.56->lavague-core<0.3.0,>=0.2.31->lavague)\n",
            "  Downloading pypdf-4.3.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index==0.10.56->lavague-core<0.3.0,>=0.2.31->lavague)\n",
            "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
            "INFO: pip is looking at multiple versions of llama-index-readers-llama-parse to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting llama-index-readers-llama-parse>=0.1.2 (from llama-index==0.10.56->lavague-core<0.3.0,>=0.2.31->lavague)\n",
            "  Downloading llama_index_readers_llama_parse-0.3.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "  Downloading llama_index_readers_llama_parse-0.2.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "  Downloading llama_index_readers_llama_parse-0.1.6-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting llama-parse>=0.4.0 (from llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.56->lavague-core<0.3.0,>=0.2.31->lavague)\n",
            "  Downloading llama_parse-0.6.4.post1-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague) (3.2.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai==0.1.11->lavague-contexts-openai<0.3.0,>=0.2.0->lavague) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai==0.1.11->lavague-contexts-openai<0.3.0,>=0.2.0->lavague) (2025.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython<8.0.0,>=7.34.0->lavague-core<0.3.0,>=0.2.31->lavague) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython<8.0.0,>=7.34.0->lavague-core<0.3.0,>=0.2.31->lavague) (0.2.13)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague) (2.27.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai==0.1.11->lavague-contexts-openai<0.3.0,>=0.2.0->lavague) (3.1.1)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium<5.0.0,>=4.18.1->lavague-drivers-selenium<0.3.0,>=0.2.12->lavague) (2.4.0)\n",
            "Collecting outcome (from trio~=0.17->selenium<5.0.0,>=4.18.1->lavague-drivers-selenium<0.3.0,>=0.2.12->lavague)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium<5.0.0,>=4.18.1->lavague-drivers-selenium<0.3.0,>=0.2.12->lavague)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague) (13.9.4)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium<5.0.0,>=4.18.1->lavague-drivers-selenium<0.3.0,>=0.2.12->lavague) (1.7.1)\n",
            "Collecting starlette<0.47.0,>=0.40.0 (from fastapi->gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague)\n",
            "  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from azure-core>=1.31.0->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai==0.1.10->lavague-contexts-openai<0.3.0,>=0.2.0->lavague) (1.17.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama-index==0.10.56->lavague-core<0.3.0,>=0.2.31->lavague) (2.6)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=2.5->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai==0.1.10->lavague-contexts-openai<0.3.0,>=0.2.0->lavague) (1.17.1)\n",
            "Requirement already satisfied: regex!=2019.02.19,!=2021.8.27,>=2015.06.24 in /usr/local/lib/python3.11/dist-packages (from dateparser>=1.1.2->htmldate>=1.8.1->trafilatura<2.0.0,>=1.9.0->lavague-core<0.3.0,>=0.2.31->lavague) (2024.11.6)\n",
            "Requirement already satisfied: tzlocal>=0.2 in /usr/local/lib/python3.11/dist-packages (from dateparser>=1.1.2->htmldate>=1.8.1->trafilatura<2.0.0,>=1.9.0->lavague-core<0.3.0,>=0.2.31->lavague) (5.3.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.52->langchain<0.2.0,>=0.1.20->lavague-core<0.3.0,>=0.2.31->lavague) (3.0.0)\n",
            "Collecting llama-cloud-services>=0.6.4 (from llama-parse>=0.4.0->llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.56->lavague-core<0.3.0,>=0.2.31->lavague)\n",
            "  Downloading llama_cloud_services-0.6.5-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: PyJWT<3,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from PyJWT[crypto]<3,>=1.0.0->msal>=1.30.0->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai==0.1.10->lavague-contexts-openai<0.3.0,>=0.2.0->lavague) (2.10.1)\n",
            "Collecting portalocker<3,>=1.4 (from msal-extensions>=1.2.0->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai==0.1.10->lavague-contexts-openai<0.3.0,>=0.2.0->lavague)\n",
            "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai==0.1.11->lavague-contexts-openai<0.3.0,>=0.2.0->lavague) (1.4.2)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai==0.1.11->lavague-contexts-openai<0.3.0,>=0.2.0->lavague) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai==0.1.11->lavague-contexts-openai<0.3.0,>=0.2.0->lavague) (0.8.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-azure-openai==0.1.11->lavague-contexts-openai<0.3.0,>=0.2.0->lavague)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=2.5->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai==0.1.10->lavague-contexts-openai<0.3.0,>=0.2.0->lavague) (2.22)\n",
            "INFO: pip is looking at multiple versions of llama-cloud-services to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting llama-cloud-services>=0.6.4 (from llama-parse>=0.4.0->llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.56->lavague-core<0.3.0,>=0.2.31->lavague)\n",
            "  Downloading llama_cloud_services-0.6.4-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting llama-parse>=0.4.0 (from llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.56->lavague-core<0.3.0,>=0.2.31->lavague)\n",
            "  Downloading llama_parse-0.6.4-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting llama-cloud-services>=0.6.3 (from llama-parse>=0.4.0->llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.56->lavague-core<0.3.0,>=0.2.31->lavague)\n",
            "  Downloading llama_cloud_services-0.6.3-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting llama-parse>=0.4.0 (from llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.56->lavague-core<0.3.0,>=0.2.31->lavague)\n",
            "  Downloading llama_parse-0.6.2-py3-none-any.whl.metadata (6.9 kB)\n",
            "INFO: pip is still looking at multiple versions of llama-cloud-services to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting llama-cloud-services>=0.6.2 (from llama-parse>=0.4.0->llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.56->lavague-core<0.3.0,>=0.2.31->lavague)\n",
            "  Downloading llama_cloud_services-0.6.2-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting llama-parse>=0.4.0 (from llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.56->lavague-core<0.3.0,>=0.2.31->lavague)\n",
            "  Downloading llama_parse-0.6.1-py3-none-any.whl.metadata (6.9 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "Collecting llama-cloud-services>=0.6.1 (from llama-parse>=0.4.0->llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.56->lavague-core<0.3.0,>=0.2.31->lavague)\n",
            "  Downloading llama_cloud_services-0.6.1-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting llama-parse>=0.4.0 (from llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.56->lavague-core<0.3.0,>=0.2.31->lavague)\n",
            "  Downloading llama_parse-0.6.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting llama-cloud-services (from llama-parse>=0.4.0->llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.56->lavague-core<0.3.0,>=0.2.31->lavague)\n",
            "  Downloading llama_cloud_services-0.6.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting llama-parse>=0.4.0 (from llama-index-readers-llama-parse>=0.1.2->llama-index==0.10.56->lavague-core<0.3.0,>=0.2.31->lavague)\n",
            "  Downloading llama_parse-0.5.20-py3-none-any.whl.metadata (6.9 kB)\n",
            "INFO: pip is looking at multiple versions of llama-parse to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading llama_parse-0.5.19-py3-none-any.whl.metadata (7.0 kB)\n",
            "  Downloading llama_parse-0.5.18-py3-none-any.whl.metadata (7.0 kB)\n",
            "  Downloading llama_parse-0.5.17-py3-none-any.whl.metadata (7.0 kB)\n",
            "  Downloading llama_parse-0.5.16-py3-none-any.whl.metadata (7.0 kB)\n",
            "  Downloading llama_parse-0.5.15-py3-none-any.whl.metadata (7.0 kB)\n",
            "  Downloading llama_parse-0.5.14-py3-none-any.whl.metadata (6.9 kB)\n",
            "  Downloading llama_parse-0.5.13-py3-none-any.whl.metadata (6.9 kB)\n",
            "INFO: pip is still looking at multiple versions of llama-parse to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading llama_parse-0.5.12-py3-none-any.whl.metadata (6.9 kB)\n",
            "  Downloading llama_parse-0.5.11-py3-none-any.whl.metadata (6.9 kB)\n",
            "  Downloading llama_parse-0.5.10-py3-none-any.whl.metadata (6.9 kB)\n",
            "  Downloading llama_parse-0.5.9-py3-none-any.whl.metadata (6.9 kB)\n",
            "  Downloading llama_parse-0.5.8-py3-none-any.whl.metadata (6.4 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading llama_parse-0.5.7-py3-none-any.whl.metadata (6.4 kB)\n",
            "  Downloading llama_parse-0.5.6-py3-none-any.whl.metadata (6.1 kB)\n",
            "  Downloading llama_parse-0.5.5-py3-none-any.whl.metadata (6.1 kB)\n",
            "  Downloading llama_parse-0.5.4-py3-none-any.whl.metadata (6.1 kB)\n",
            "  Downloading llama_parse-0.5.3-py3-none-any.whl.metadata (4.5 kB)\n",
            "  Downloading llama_parse-0.5.2-py3-none-any.whl.metadata (4.5 kB)\n",
            "  Downloading llama_parse-0.5.1-py3-none-any.whl.metadata (4.5 kB)\n",
            "  Downloading llama_parse-0.5.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "  Downloading llama_parse-0.4.9-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio==4.39.0->lavague-gradio<0.3.0,>=0.2.8->lavague) (0.1.2)\n",
            "Downloading lavague-1.1.19-py3-none-any.whl (8.4 kB)\n",
            "Downloading lavague_contexts_openai-0.2.4-py3-none-any.whl (2.5 kB)\n",
            "Downloading llama_index_embeddings_azure_openai-0.1.11-py3-none-any.whl (3.3 kB)\n",
            "Downloading llama_index_llms_azure_openai-0.1.10-py3-none-any.whl (5.1 kB)\n",
            "Downloading llama_index_multi_modal_llms_azure_openai-0.1.4-py3-none-any.whl (3.7 kB)\n",
            "Downloading lavague_core-0.2.35-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index-0.10.56-py3-none-any.whl (6.8 kB)\n",
            "Downloading llama_index_core-0.10.56-py3-none-any.whl (15.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.5/15.5 MB\u001b[0m \u001b[31m83.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lavague_drivers_selenium-0.2.15-py3-none-any.whl (12 kB)\n",
            "Downloading lavague_gradio-0.2.8-py3-none-any.whl (3.5 kB)\n",
            "Downloading gradio-4.39.0-py3-none-any.whl (12.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m88.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.1.1-py3-none-any.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.2/318.2 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Downloading langchain-0.1.20-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_embeddings_openai-0.1.11-py3-none-any.whl (6.3 kB)\n",
            "Downloading llama_index_llms_openai-0.1.26-py3-none-any.whl (11 kB)\n",
            "Downloading llama_index_multi_modal_llms_openai-0.1.9-py3-none-any.whl (5.9 kB)\n",
            "Downloading llama_index_retrievers_bm25-0.1.5-py3-none-any.whl (2.8 kB)\n",
            "Downloading lxml_html_clean-0.1.1-py3-none-any.whl (11 kB)\n",
            "Downloading selenium-4.29.0-py3-none-any.whl (9.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m112.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.3.0-py3-none-any.whl (25 kB)\n",
            "Downloading trafilatura-1.12.2-py3-none-any.whl (132 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.2/132.2 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading azure_identity-1.21.0-py3-none-any.whl (189 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.2/189.2 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading courlan-1.3.2-py3-none-any.whl (33 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading htmldate-1.9.3-py3-none-any.whl (31 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading justext-3.0.2-py2.py3-none-any.whl (837 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m837.9/837.9 kB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.0.38-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.1.53-py3-none-any.whl (303 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.1/303.1 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.0.2-py3-none-any.whl (23 kB)\n",
            "Downloading langsmith-0.1.147-py3-none-any.whl (311 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.8/311.8 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_agent_openai-0.2.9-py3-none-any.whl (13 kB)\n",
            "Downloading llama_index_cli-0.1.13-py3-none-any.whl (27 kB)\n",
            "Downloading llama_index_indices_managed_llama_cloud-0.2.7-py3-none-any.whl (9.5 kB)\n",
            "Downloading llama_index_legacy-0.9.48.post4-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_program_openai-0.1.6-py3-none-any.whl (5.2 kB)\n",
            "Downloading llama_index_question_gen_openai-0.1.3-py3-none-any.whl (2.9 kB)\n",
            "Downloading llama_index_readers_file-0.1.33-py3-none-any.whl (38 kB)\n",
            "Downloading llama_index_readers_llama_parse-0.1.6-py3-none-any.whl (2.5 kB)\n",
            "Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
            "Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-10.4.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m123.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Downloading ruff-0.9.10-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m104.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading trio-0.29.0-py3-none-any.whl (492 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m492.9/492.9 kB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.115.11-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading azure_core-1.32.0-py3-none-any.whl (198 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.9/198.9 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dateparser-1.2.1-py3-none-any.whl (295 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.7/295.7 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Downloading llama_cloud-0.1.14-py3-none-any.whl (261 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.7/261.7 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_parse-0.4.9-py3-none-any.whl (9.4 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading msal-1.31.1-py3-none-any.whl (113 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.2/113.2 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading msal_extensions-1.2.0-py3-none-any.whl (19 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading pypdf-4.3.1-py3-none-any.whl (295 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.46.1-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tld-0.13-py2.py3-none-any.whl (263 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m263.8/263.8 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading websockets-11.0.3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.6/130.6 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: striprtf, pydub, dirtyjson, wsproto, websockets, uvicorn, tomlkit, tld, tenacity, semantic-version, ruff, rank-bm25, python-multipart, python-dateutil, pypdf, portalocker, pillow, packaging, outcome, mypy-extensions, markupsafe, lxml-html-clean, jedi, ffmpy, aiofiles, typing-inspect, trio, tiktoken, starlette, marshmallow, dateparser, courlan, azure-core, trio-websocket, llama-cloud, langsmith, justext, htmldate, gradio-client, fastapi, dataclasses-json, trafilatura, selenium, msal, llama-index-legacy, llama-index-core, langchain-core, gradio, msal-extensions, llama-parse, llama-index-retrievers-bm25, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, langchain-text-splitters, langchain-community, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, langchain, azure-identity, llama-index-program-openai, llama-index-llms-azure-openai, llama-index-question-gen-openai, llama-index-multi-modal-llms-azure-openai, llama-index-embeddings-azure-openai, llama-index, lavague-core, lavague-gradio, lavague-drivers-selenium, lavague-contexts-openai, lavague\n",
            "  Attempting uninstall: websockets\n",
            "    Found existing installation: websockets 14.2\n",
            "    Uninstalling websockets-14.2:\n",
            "      Successfully uninstalled websockets-14.2\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "  Attempting uninstall: python-dateutil\n",
            "    Found existing installation: python-dateutil 2.8.2\n",
            "    Uninstalling python-dateutil-2.8.2:\n",
            "      Successfully uninstalled python-dateutil-2.8.2\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: pillow 11.1.0\n",
            "    Uninstalling pillow-11.1.0:\n",
            "      Successfully uninstalled pillow-11.1.0\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.2\n",
            "    Uninstalling packaging-24.2:\n",
            "      Successfully uninstalled packaging-24.2\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "  Attempting uninstall: langsmith\n",
            "    Found existing installation: langsmith 0.3.13\n",
            "    Uninstalling langsmith-0.3.13:\n",
            "      Successfully uninstalled langsmith-0.3.13\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.43\n",
            "    Uninstalling langchain-core-0.3.43:\n",
            "      Successfully uninstalled langchain-core-0.3.43\n",
            "  Attempting uninstall: langchain-text-splitters\n",
            "    Found existing installation: langchain-text-splitters 0.3.6\n",
            "    Uninstalling langchain-text-splitters-0.3.6:\n",
            "      Successfully uninstalled langchain-text-splitters-0.3.6\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.20\n",
            "    Uninstalling langchain-0.3.20:\n",
            "      Successfully uninstalled langchain-0.3.20\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "google-genai 1.4.0 requires websockets<15.0dev,>=13.0, but you have websockets 11.0.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiofiles-23.2.1 azure-core-1.32.0 azure-identity-1.21.0 courlan-1.3.2 dataclasses-json-0.6.7 dateparser-1.2.1 dirtyjson-1.0.8 fastapi-0.115.11 ffmpy-0.5.0 gradio-4.39.0 gradio-client-1.1.1 htmldate-1.9.3 jedi-0.19.2 justext-3.0.2 langchain-0.1.20 langchain-community-0.0.38 langchain-core-0.1.53 langchain-text-splitters-0.0.2 langsmith-0.1.147 lavague-1.1.19 lavague-contexts-openai-0.2.4 lavague-core-0.2.35 lavague-drivers-selenium-0.2.15 lavague-gradio-0.2.8 llama-cloud-0.1.14 llama-index-0.10.56 llama-index-agent-openai-0.2.9 llama-index-cli-0.1.13 llama-index-core-0.10.56 llama-index-embeddings-azure-openai-0.1.11 llama-index-embeddings-openai-0.1.11 llama-index-indices-managed-llama-cloud-0.2.7 llama-index-legacy-0.9.48.post4 llama-index-llms-azure-openai-0.1.10 llama-index-llms-openai-0.1.26 llama-index-multi-modal-llms-azure-openai-0.1.4 llama-index-multi-modal-llms-openai-0.1.9 llama-index-program-openai-0.1.6 llama-index-question-gen-openai-0.1.3 llama-index-readers-file-0.1.33 llama-index-readers-llama-parse-0.1.6 llama-index-retrievers-bm25-0.1.5 llama-parse-0.4.9 lxml-html-clean-0.1.1 markupsafe-2.1.5 marshmallow-3.26.1 msal-1.31.1 msal-extensions-1.2.0 mypy-extensions-1.0.0 outcome-1.3.0.post0 packaging-23.2 pillow-10.4.0 portalocker-2.10.1 pydub-0.25.1 pypdf-4.3.1 python-dateutil-2.9.0.post0 python-multipart-0.0.20 rank-bm25-0.2.2 ruff-0.9.10 selenium-4.29.0 semantic-version-2.10.0 starlette-0.46.1 striprtf-0.0.26 tenacity-8.3.0 tiktoken-0.9.0 tld-0.13 tomlkit-0.12.0 trafilatura-1.12.2 trio-0.29.0 trio-websocket-0.12.2 typing-inspect-0.9.0 uvicorn-0.34.0 websockets-11.0.3 wsproto-1.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "dateutil"
                ]
              },
              "id": "c03164cb96c842679f7062fd2d66ae16"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install lavague"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsPFyYrJlJb4"
      },
      "source": [
        "We will need to set our OpenAI Key as a Colab secret (see the key icon on the left-hand side of the Colab notebook) named 'OPENAI_API_KEY' and then convert it to an environment variable with the same name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "szM8iq3LINtq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "8293f110-c90c-408d-d116-da7a277d2f63"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SecretNotFoundError",
          "evalue": "Secret OPENAI_API_KEY does not exist.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSecretNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-3973237eb174>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mIN_COLAB\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"OPENAI_API_KEY\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'OPENAI_API_KEY'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"OPENAI_API_KEY\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'OPENAI_API_KEY'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/userdata.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'exists'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mSecretNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'access'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mNotebookAccessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSecretNotFoundError\u001b[0m: Secret OPENAI_API_KEY does not exist."
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Check if running in Google Colab\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "else:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
        "\n",
        "    from google.colab import userdata\n",
        "userdata.get('secretName')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhHF9m4zd8mJ"
      },
      "source": [
        "## ActionEngine\n",
        "\n",
        "**An WebAgent is made up of two components: an `ActionEngine` and a `WorldModel`.**\n",
        "\n",
        "Let's start by initializing an `ActionEngine`, which is responsible for generating automation code for text instructions and executing them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1sWzXVMd8mJ"
      },
      "outputs": [],
      "source": [
        "from lavague.core import ActionEngine\n",
        "from lavague.drivers.selenium import SeleniumDriver\n",
        "\n",
        "selenium_driver = SeleniumDriver()\n",
        "action_engine = ActionEngine(selenium_driver)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s173cCGjd8mK"
      },
      "source": [
        "# World model\n",
        "\n",
        "Next, we will initialize our `WorldModel`, providing it with examples of global objectives for actions and the desired thought process and reasoning we wish it to replicate to generate the next instruction that needs to be passed to the `ActionEngine`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "obgdqBPyd8mM"
      },
      "outputs": [],
      "source": [
        "from lavague.core import WorldModel\n",
        "\n",
        "world_model = WorldModel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umuDUrJNbsGe"
      },
      "source": [
        "# WebAgent Demo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3M6DerDyd8mM"
      },
      "source": [
        "We can now use these two elements to initialize a `WebAgent` and start playing with it!\n",
        "\n",
        "In the following example, we show how our agent can achieve a user-defined goal, here going on the quicktour of Hugging Face's PEFT framework for model finetuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ckkvh1BF19J"
      },
      "outputs": [],
      "source": [
        "from lavague.core.agents import WebAgent\n",
        "\n",
        "agent = WebAgent(world_model, action_engine)\n",
        "\n",
        "agent.get(\"https://huggingface.co/docs\")\n",
        "agent.run(\"Go on the quicktour of PEFT\", display=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQSCeJBQlJb_"
      },
      "source": [
        "### Interactive demo mode\n",
        "\n",
        "You can also launch an interactive Gradio interface for using the agent with the `agent.demo()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHePOrRLlJcA"
      },
      "outputs": [],
      "source": [
        "from lavague.core.agents import WebAgent\n",
        "\n",
        "driver = SeleniumDriver(headless=True)\n",
        "action_engine = ActionEngine(driver)\n",
        "world_model = WorldModel()\n",
        "\n",
        "# Create Web Agent\n",
        "agent = WebAgent(world_model, action_engine)\n",
        "\n",
        "# Set URL\n",
        "agent.get(\"https://huggingface.co/docs\")\n",
        "\n",
        "# Launch the agent in the Agent Gradio Demo mode\n",
        "agent.demo(\"Go on the quicktour of PEFT\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}